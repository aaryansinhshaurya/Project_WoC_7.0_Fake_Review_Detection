{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lXdW6rO0-1c",
        "outputId": "beebbe61-2390-42a7-d9ea-06243727d340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'fakeReviewData.csv'\n",
        "raw_data = pd.read_csv(file_path) #reading the csv file\n",
        "\n",
        "# Displaying the first few rows of the raw data\n",
        "print(raw_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA CLEANING"
      ],
      "metadata": {
        "id": "9z4FQ17858NY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values by removing rows with any NaN values\n",
        "clean_data = raw_data.dropna()\n",
        "\n",
        "# Removing the duplicate rows\n",
        "clean_data = clean_data.drop_duplicates()\n",
        "\n",
        "# Displaying the first few rows of the cleaned data\n",
        "print(clean_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncnCz_3-1oMa",
        "outputId": "dd4211df-550f-4c8e-f0fc-f57bbc9bf35e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  Love this!  Well made, sturdy, and very comfor...  \n",
            "1  love it, a great upgrade from the original.  I...  \n",
            "2  This pillow saved my back. I love the look and...  \n",
            "3  Missing information on how to use it, but it i...  \n",
            "4  Very nice set. Good quality. We have had the s...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # NORMALIZING TEXT"
      ],
      "metadata": {
        "id": "qgHFBndA6PPF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "\n",
        "    # Converting text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation, special characters, and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Applying the normalization function to the text column\n",
        "clean_data['text_'] = clean_data['text_'].apply(normalize_text)\n",
        "\n",
        "# Displaying the first few rows of the Normalized data\n",
        "print(clean_data.head())\n"
      ],
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \n",
            "0  love this  well made sturdy and very comfortab...  \n",
            "1  love it a great upgrade from the original  ive...  \n",
            "2  this pillow saved my back i love the look and ...  \n",
            "3  missing information on how to use it but it is...  \n",
            "4  very nice set good quality we have had the set...  \n"
          ]
        }
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcCs-Msk1Pmt",
        "outputId": "fdab5043-0a84-4cfd-bcf0-2bf1904ced24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZATION"
      ],
      "metadata": {
        "id": "XuA8hEOr6mNi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Downloading the NLTK tokenizer models\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenizing the 'text_' column using NLTK's word_tokenize function\n",
        "clean_data['tokens'] = clean_data['text_'].apply(word_tokenize)\n",
        "\n",
        "# Displaying the first few rows of the Tokenized data\n",
        "print(clean_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bRGawvi1aaL",
        "outputId": "27ad93d6-194b-4968-ac65-c3f7a3e970ea"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, this, well, made, sturdy, and, very, co...  \n",
            "1  [love, it, a, great, upgrade, from, the, origi...  \n",
            "2  [this, pillow, saved, my, back, i, love, the, ...  \n",
            "3  [missing, information, on, how, to, use, it, b...  \n",
            "4  [very, nice, set, good, quality, we, have, had...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVING THE STOP WORDS"
      ],
      "metadata": {
        "id": "0ZMWSiIz645S"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading stopwords list and tokenizer models\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for word in tokens:\n",
        "\n",
        "        # Check if the word is not in the set of stopwords\n",
        "        if word not in stop_words:\n",
        "\n",
        "            # Add the word to the filtered list if it's not a stopword\n",
        "            filtered_tokens.append(word)\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "clean_data['tokens'] = clean_data['tokens'].apply(remove_stopwords)\n",
        "\n",
        "# Displaying the first few rows after the stop words have been removed\n",
        "print(clean_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ge7BVUh2wwX",
        "outputId": "5cdebcbf-13fd-46d1-d2f4-14a8b4b51af8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, ive, mine, co...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4       [nice, set, good, quality, set, two, months]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LEMATIZATION"
      ],
      "metadata": {
        "id": "Yi52xUWX7Jnd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Downloading the WordNet lemmatizer models\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def apply_lemmatization(tokens):\n",
        "\n",
        "    # Initialize an empty list to store lemmatized tokens\n",
        "    lemmatized_tokens = []\n",
        "\n",
        "    # Iterate over each token in the list\n",
        "    for word in tokens:\n",
        "\n",
        "        # Lemmatize the word and add it to the lemmatized_tokens list\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(word,pos='n'))\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "# Applying lemmatization to the 'tokens' column\n",
        "clean_data['tokens'] = clean_data['tokens'].apply(apply_lemmatization)\n",
        "\n",
        "# Displaying the first few rows of the lemmatized data\n",
        "print(clean_data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wYuyCHr20W0",
        "outputId": "96b7ddab-3dee-40b4-822f-7360b11746cf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \n",
            "0  [love, well, made, sturdy, comfortable, love, ...  \n",
            "1  [love, great, upgrade, original, ive, mine, co...  \n",
            "2    [pillow, saved, back, love, look, feel, pillow]  \n",
            "3  [missing, information, use, great, product, pr...  \n",
            "4        [nice, set, good, quality, set, two, month]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec TRANSFORMATION"
      ],
      "metadata": {
        "id": "3YOeQj_I7mqq"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Step 1: Prepare Corpus\n",
        "# Extract the 'tokens' column as a list of tokenized sentences\n",
        "corpus = clean_data['tokens'].tolist()\n",
        "\n",
        "# Step 2: Train Word2Vec Model\n",
        "# Initialize and train the Word2Vec model\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Step 3: Create Sentence Vectors\n",
        "# Function to compute the average word vector for a list of tokens\n",
        "\n",
        "def get_sentence_vector(tokens, model):\n",
        "\n",
        "  # Initialize an empty list to store word vectors\n",
        "  vectors = []\n",
        "\n",
        "  # Loop through each word in tokens\n",
        "  for word in tokens:\n",
        "     # Check if the word exists in the Word2Vec model's vocabulary\n",
        "      if word in model.wv:\n",
        "          # Retrieve the word vector and append it to the list\n",
        "          vectors.append(model.wv[word])\n",
        "\n",
        "\n",
        "  if vectors:\n",
        "      return sum(vectors) / len(vectors)\n",
        "  else:\n",
        "      return [0] * model.vector_size  # Return zero vector for empty tokens\n",
        "\n",
        "\n",
        "\n",
        "# Applying the function to compute sentence vectors\n",
        "clean_data['sentence_vector'] = clean_data['tokens'].apply(lambda x: get_sentence_vector(x, model))\n",
        "\n",
        "# Step 4: Save Processed Data\n",
        "clean_data.to_csv(\"final_fake_review_data.csv\", index=False)\n",
        "\n",
        "# Downloading the final processed file\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"final_fake_review_data.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_DKOhGcP291n",
        "outputId": "70a68ed4-18cf-4a9b-af09-ae6f1eadcec3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2fc23f70-4186-436c-8310-9566469d276f\", \"final_fake_review_data.csv\", 78982017)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DONE"
      ],
      "metadata": {
        "id": "sZBBlIc37t8p"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first few rows of the data\n",
        "data = pd.read_csv('final_fake_review_data.csv');\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ChtH_r73TrQ",
        "outputId": "3a39aaf4-e6ab-4ef9-e278-c0954d618c3d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             category  rating label  \\\n",
            "0  Home_and_Kitchen_5     5.0    CG   \n",
            "1  Home_and_Kitchen_5     5.0    CG   \n",
            "2  Home_and_Kitchen_5     5.0    CG   \n",
            "3  Home_and_Kitchen_5     1.0    CG   \n",
            "4  Home_and_Kitchen_5     5.0    CG   \n",
            "\n",
            "                                               text_  \\\n",
            "0  love this  well made sturdy and very comfortab...   \n",
            "1  love it a great upgrade from the original  ive...   \n",
            "2  this pillow saved my back i love the look and ...   \n",
            "3  missing information on how to use it but it is...   \n",
            "4  very nice set good quality we have had the set...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  ['love', 'well', 'made', 'sturdy', 'comfortabl...   \n",
            "1  ['love', 'great', 'upgrade', 'original', 'ive'...   \n",
            "2  ['pillow', 'saved', 'back', 'love', 'look', 'f...   \n",
            "3  ['missing', 'information', 'use', 'great', 'pr...   \n",
            "4  ['nice', 'set', 'good', 'quality', 'set', 'two...   \n",
            "\n",
            "                                     sentence_vector  \n",
            "0  [ 1.13825691e+00 -1.23274910e+00  4.99514729e-...  \n",
            "1  [ 0.48420948 -0.3366535  -0.3855758  -0.085326...  \n",
            "2  [ 0.7635173   0.391246    0.04960359 -0.212154...  \n",
            "3  [ 0.3463842  -0.55107814 -0.9666736   0.293847...  \n",
            "4  [ 0.72801715 -0.31683007 -0.23926404  0.128459...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('bad')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyQ-pB8_3ZRP",
        "outputId": "41ebc4d4-591c-4b48-9f9f-25f8c0c9b85a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('thats', 0.7340174913406372),\n",
              " ('guess', 0.7258275747299194),\n",
              " ('terrible', 0.7166757583618164),\n",
              " ('horrible', 0.7091423273086548),\n",
              " ('sad', 0.7072671055793762),\n",
              " ('hate', 0.7050395607948303),\n",
              " ('expecting', 0.703579843044281),\n",
              " ('saying', 0.6955850720405579),\n",
              " ('reformed', 0.6738972663879395),\n",
              " ('stupid', 0.6734278202056885)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}